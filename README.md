# Multi-Head Self-Attention Transformer Implementation

## About
This project is a simple implementation of the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762) using [Pytorch](https://pytorch.org/). 
Some of the code here are inspired by [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/) implementation.

## Model Architecture
This project's model follows the exact model architecture in the paper.
![image](https://github.com/lloydaxeph/multi_head_attention_transformer/assets/158691653/7c382a8d-e123-41e6-aef1-aadbe919ac21)

## Applications
I created a simple [German-to-English Translator](https://github.com/lloydaxeph/german_english_translator) using this model to demonstrate how can we train and test this model. 
